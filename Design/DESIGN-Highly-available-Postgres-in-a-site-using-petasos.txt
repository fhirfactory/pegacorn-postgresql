Highly available postgres in a site using petasos
=================================================

In each site where a database is required (currently site-a and site-b for fhirplace), 
there will be at least one 100GB persistent storage disk attached to a VM (the VMs are denoted by a Kubernetes node label e.g. fhirplace=Yes or hestia=Yes).  
For each Kubernetes node that has the label, a single pegacorn-postgres pod will be deployed to it by the Kubernetes DaemonSet (which ensures that only 1 pod 
is deployed to each labelled node) e.g. fhirplace, hestia-dam.  

The pegacorn-postgres pod accesses the persistent storage disk via a hostPath e.g. /kube-vols/fhirplace/data which is a sub folder of the disk mount 
e.g. /kube-vols/fhirplace.  In case there are more than 1 instances of a given database in a site (for increased redundancy, the benefit is if we are down to just
one site, even if one of the nodes hosting the database goes down, the other node(s) hosting the database can continue), another Kubernetes DaemonSet of
the Java code persisting to postgres is required to ensure that if a given instance of the Java persistence code is called on a given node, it will
persist using the postgres instance on that node.  In the case of fhirplace, it is the hapi-fhir-jpaserver that is the Java code persisting.  Petasos will be
added to the Java code doing the persisting so if any of the pods/nodes are asked to persist, petasos will inform all other nodes to perform the equivalent
logical (e.g. Internal Ids generated by postgres will most probably be different on the different pods/nodes for the same business object) persistence.


VM 1					        					... 						VM n
- Storage Disk																	- Storage Disk
       ^									        								^
- hostPath																		- hostPath
       ^									      									^
- pegacorn-postgres pod 1 														- pegacorn-postgres pod n
       ^									        								^
- hostPort																		- hostPort
       ^									        								^
- hapi-fhir-jpaserver pod 1			<-- Petasos logical replication -->			- hapi-fhir-jpaserver pod n
	       ^			      		  with ALL nodes having to action		     		^
		   |																			|
	       |----------------------------------------------------------------------------|
												^
								hapi-fhir-jpaserver Kubernetes service
								     (Round robin load balancer)


To allow for application support, we want to be able to connect to a specific pegacorn-postgres instance running on a specific VM.  The hostPort solution 
achieves this.  Please note that for Kubernetes clusters hosted by Docker for Windows, hostPort didn't appear to work (see 
https://github.com/docker/for-win/issues/5406), so instead this was implemented as a NodePort Kubernetes service, which in a single node/host Kubenetes 
cluster has the same behaviour as hostPort.

The DaemonSet pods hosting the Java code persisting to postgres (e.g. hapi-fhir) need to support the postgres dataSourceUrl, to lookup parts from system 
environment variables e.g. [ENV:MY_HOST_IP], where the system environment variable MY_HOST_IP is injected into the pod by instructions in the definition 
of the Kubernetes DaemonSet.

I considered combining the pegacorn-postgres and hapi-fhir-jpaserver containers into a single pod, but this has the following disadvantages:
* Not possible to upgrade the containers independently of one another
* If one container stops working, both containers are restarted
* From https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/#discussion and 
  https://linchpiner.github.io/k8s-multi-container-pods.html#_use_cases_for_multi_container_pods, multi container pods are usually used to allow helper 
  applications (e.g. data pullers, data pushers, and proxies) to assist a primary application, but that is not the case here
* Still need to create the hostPort
But has the advantages of
* tightly coupling the healthiness of the containers together, as if hapi-fhir down is down, postgres is not available to calling code.  Also if postgres 
  is down, hapi-fhir is unable to fulfill its functional requirement (although with petasos added to hapi-fhir, small outages of postgres can be absorbed 
  and the calling code would be unaware, apart from a slower response time)
